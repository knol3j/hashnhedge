"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.GeminiChatModel = void 0;
const openai_1 = require("@aigne/openai");
const GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai";
const GEMINI_DEFAULT_CHAT_MODEL = "gemini-2.0-flash";
/**
 * Implementation of the ChatModel interface for Google's Gemini API
 *
 * This model uses OpenAI-compatible API format to interact with Google's Gemini models,
 * providing access to models like Gemini 1.5 and Gemini 2.0.
 *
 * @example
 * Here's how to create and use a Gemini chat model:
 * {@includeCode ../test/gemini-chat-model.test.ts#example-gemini-chat-model}
 *
 * @example
 * Here's an example with streaming response:
 * {@includeCode ../test/gemini-chat-model.test.ts#example-gemini-chat-model-streaming}
 */
class GeminiChatModel extends openai_1.OpenAIChatModel {
    constructor(options) {
        super({
            ...options,
            model: options?.model || GEMINI_DEFAULT_CHAT_MODEL,
            baseURL: options?.baseURL || GEMINI_BASE_URL,
        });
    }
    apiKeyEnvName = "GEMINI_API_KEY";
    supportsToolsUseWithJsonSchema = false;
    supportsParallelToolCalls = false;
    supportsToolStreaming = false;
    async getRunMessages(input) {
        const messages = await super.getRunMessages(input);
        const lastMessage = messages.at(-1);
        if (lastMessage?.role === "system") {
            lastMessage.role = "user"; // Ensure the last message is from the user
        }
        return messages;
    }
}
exports.GeminiChatModel = GeminiChatModel;
