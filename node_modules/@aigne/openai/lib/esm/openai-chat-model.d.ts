import { type AgentProcessResult, ChatModel, type ChatModelInput, type ChatModelInputMessage, type ChatModelInputTool, type ChatModelOptions, type ChatModelOutput } from "@aigne/core";
import { type PromiseOrValue } from "@aigne/core/utils/type-utils.js";
import type { ClientOptions, OpenAI } from "openai";
import type { ChatCompletionMessageParam, ChatCompletionTool } from "openai/resources";
import { z } from "zod";
export interface OpenAIChatModelCapabilities {
    supportsNativeStructuredOutputs: boolean;
    supportsEndWithSystemMessage: boolean;
    supportsToolsUseWithJsonSchema: boolean;
    supportsParallelToolCalls: boolean;
    supportsToolsEmptyParameters: boolean;
    supportsToolStreaming: boolean;
    supportsTemperature: boolean;
}
/**
 * Configuration options for OpenAI Chat Model
 */
export interface OpenAIChatModelOptions {
    /**
     * API key for OpenAI API
     *
     * If not provided, will look for OPENAI_API_KEY in environment variables
     */
    apiKey?: string;
    /**
     * Base URL for OpenAI API
     *
     * Useful for proxies or alternate endpoints
     */
    baseURL?: string;
    /**
     * OpenAI model to use
     *
     * Defaults to 'gpt-4o-mini'
     */
    model?: string;
    /**
     * Additional model options to control behavior
     */
    modelOptions?: ChatModelOptions;
    /**
     * Client options for OpenAI API
     */
    clientOptions?: Partial<ClientOptions>;
}
/**
 * @hidden
 */
export declare const openAIChatModelOptionsSchema: z.ZodObject<{
    apiKey: z.ZodOptional<z.ZodString>;
    baseURL: z.ZodOptional<z.ZodString>;
    model: z.ZodOptional<z.ZodString>;
    modelOptions: z.ZodOptional<z.ZodObject<{
        model: z.ZodOptional<z.ZodString>;
        temperature: z.ZodOptional<z.ZodNumber>;
        topP: z.ZodOptional<z.ZodNumber>;
        frequencyPenalty: z.ZodOptional<z.ZodNumber>;
        presencePenalty: z.ZodOptional<z.ZodNumber>;
        parallelToolCalls: z.ZodDefault<z.ZodOptional<z.ZodBoolean>>;
    }, "strip", z.ZodTypeAny, {
        parallelToolCalls: boolean;
        model?: string | undefined;
        temperature?: number | undefined;
        topP?: number | undefined;
        frequencyPenalty?: number | undefined;
        presencePenalty?: number | undefined;
    }, {
        model?: string | undefined;
        temperature?: number | undefined;
        topP?: number | undefined;
        frequencyPenalty?: number | undefined;
        presencePenalty?: number | undefined;
        parallelToolCalls?: boolean | undefined;
    }>>;
}, "strip", z.ZodTypeAny, {
    apiKey?: string | undefined;
    baseURL?: string | undefined;
    model?: string | undefined;
    modelOptions?: {
        parallelToolCalls: boolean;
        model?: string | undefined;
        temperature?: number | undefined;
        topP?: number | undefined;
        frequencyPenalty?: number | undefined;
        presencePenalty?: number | undefined;
    } | undefined;
}, {
    apiKey?: string | undefined;
    baseURL?: string | undefined;
    model?: string | undefined;
    modelOptions?: {
        model?: string | undefined;
        temperature?: number | undefined;
        topP?: number | undefined;
        frequencyPenalty?: number | undefined;
        presencePenalty?: number | undefined;
        parallelToolCalls?: boolean | undefined;
    } | undefined;
}>;
/**
 * Implementation of the ChatModel interface for OpenAI's API
 *
 * This model provides access to OpenAI's capabilities including:
 * - Text generation
 * - Tool use with parallel tool calls
 * - JSON structured output
 * - Image understanding
 *
 * Default model: 'gpt-4o-mini'
 *
 * @example
 * Here's how to create and use an OpenAI chat model:
 * {@includeCode ../test/openai-chat-model.test.ts#example-openai-chat-model}
 *
 * @example
 * Here's an example with streaming response:
 * {@includeCode ../test/openai-chat-model.test.ts#example-openai-chat-model-stream}
 */
export declare class OpenAIChatModel extends ChatModel {
    options?: OpenAIChatModelOptions | undefined;
    constructor(options?: OpenAIChatModelOptions | undefined);
    /**
     * @hidden
     */
    protected _client?: OpenAI;
    protected apiKeyEnvName: string;
    protected apiKeyDefault: string | undefined;
    protected supportsNativeStructuredOutputs: boolean;
    protected supportsToolsUseWithJsonSchema: boolean;
    protected supportsParallelToolCalls: boolean;
    protected supportsToolsEmptyParameters: boolean;
    protected supportsToolStreaming: boolean;
    protected supportsTemperature: boolean;
    get client(): OpenAI;
    get credential(): {
        url: string | undefined;
        apiKey: string | undefined;
        model: string;
    };
    get modelOptions(): ChatModelOptions | undefined;
    /**
     * Process the input and generate a response
     * @param input The input to process
     * @returns The generated response
     */
    process(input: ChatModelInput): PromiseOrValue<AgentProcessResult<ChatModelOutput>>;
    private ajv;
    private _process;
    private getParallelToolCalls;
    protected getRunMessages(input: ChatModelInput): Promise<ChatCompletionMessageParam[]>;
    private getRunResponseFormat;
    private requestStructuredOutput;
    private extractResultFromStream;
}
/**
 * @hidden
 */
export declare function contentsFromInputMessages(messages: ChatModelInputMessage[]): Promise<ChatCompletionMessageParam[]>;
/**
 * @hidden
 */
export declare function toolsFromInputTools(tools?: ChatModelInputTool[], options?: {
    addTypeToEmptyParameters?: boolean;
}): ChatCompletionTool[] | undefined;
/**
 * @hidden
 */
export declare function jsonSchemaToOpenAIJsonSchema(schema: Record<string, unknown>): Record<string, unknown>;
