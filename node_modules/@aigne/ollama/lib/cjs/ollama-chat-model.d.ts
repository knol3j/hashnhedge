import { OpenAIChatModel, type OpenAIChatModelOptions } from "@aigne/openai";
/**
 * Implementation of the ChatModel interface for Ollama
 *
 * This model allows you to run open-source LLMs locally using Ollama,
 * with an OpenAI-compatible API interface.
 *
 * Default model: 'llama3.2'
 *
 * @example
 * Here's how to create and use an Ollama chat model:
 * {@includeCode ../test/ollama-chat-model.test.ts#example-ollama-chat-model}
 *
 * @example
 * Here's an example with streaming response:
 * {@includeCode ../test/ollama-chat-model.test.ts#example-ollama-chat-model-streaming}
 */
export declare class OllamaChatModel extends OpenAIChatModel {
    constructor(options?: OpenAIChatModelOptions);
    protected apiKeyEnvName: string;
    protected apiKeyDefault: string;
}
