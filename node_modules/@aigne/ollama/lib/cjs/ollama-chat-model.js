"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.OllamaChatModel = void 0;
const openai_1 = require("@aigne/openai");
const OLLAMA_DEFAULT_BASE_URL = "http://localhost:11434/v1";
const OLLAMA_DEFAULT_CHAT_MODEL = "llama3.2";
/**
 * Implementation of the ChatModel interface for Ollama
 *
 * This model allows you to run open-source LLMs locally using Ollama,
 * with an OpenAI-compatible API interface.
 *
 * Default model: 'llama3.2'
 *
 * @example
 * Here's how to create and use an Ollama chat model:
 * {@includeCode ../test/ollama-chat-model.test.ts#example-ollama-chat-model}
 *
 * @example
 * Here's an example with streaming response:
 * {@includeCode ../test/ollama-chat-model.test.ts#example-ollama-chat-model-streaming}
 */
class OllamaChatModel extends openai_1.OpenAIChatModel {
    constructor(options) {
        super({
            ...options,
            model: options?.model || OLLAMA_DEFAULT_CHAT_MODEL,
            baseURL: options?.baseURL || process.env.OLLAMA_BASE_URL || OLLAMA_DEFAULT_BASE_URL,
        });
    }
    apiKeyEnvName = "OLLAMA_API_KEY";
    apiKeyDefault = "ollama";
}
exports.OllamaChatModel = OllamaChatModel;
